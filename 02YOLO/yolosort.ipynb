{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-image in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (0.21.0)\n",
      "Requirement already satisfied: numpy>=1.21.1 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from scikit-image) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.8 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from scikit-image) (1.10.1)\n",
      "Requirement already satisfied: networkx>=2.8 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from scikit-image) (2.8.8)\n",
      "Requirement already satisfied: pillow>=9.0.1 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from scikit-image) (10.0.1)\n",
      "Requirement already satisfied: imageio>=2.27 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from scikit-image) (2.32.0)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from scikit-image) (2023.7.10)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: packaging>=21 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from scikit-image) (23.2)\n",
      "Requirement already satisfied: lazy_loader>=0.2 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from scikit-image) (0.3)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: filterpy in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (1.4.5)\n",
      "Requirement already satisfied: numpy in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from filterpy) (1.24.4)\n",
      "Requirement already satisfied: scipy in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from filterpy) (1.10.1)\n",
      "Requirement already satisfied: matplotlib in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from filterpy) (3.7.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from matplotlib->filterpy) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from matplotlib->filterpy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from matplotlib->filterpy) (4.44.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from matplotlib->filterpy) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from matplotlib->filterpy) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from matplotlib->filterpy) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from matplotlib->filterpy) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from matplotlib->filterpy) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from matplotlib->filterpy) (6.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib->filterpy) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->filterpy) (1.16.0)\n",
      "Requirement already satisfied: pandas in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: moviepy in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (1.0.3)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from moviepy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from moviepy) (2.31.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from moviepy) (1.24.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from moviepy) (2.32.0)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from moviepy) (0.4.9)\n",
      "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from imageio<3.0,>=2.5->moviepy) (10.0.1)\n",
      "Requirement already satisfied: setuptools in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from imageio-ffmpeg>=0.2.0->moviepy) (68.2.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/owo/anaconda3/envs/pose/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-image\n",
    "! pip install filterpy\n",
    "! pip install pandas\n",
    "! pip install moviepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 동영상 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'mevid.mp4'\n",
    "#video_path = '../OUTPUT/backgone.avi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sys\n",
    "\n",
    "def construct_yolo_v3():\n",
    "    f=open('coco_names.txt', 'r')\n",
    "    class_names=[line.strip() for line in f.readlines()]\n",
    "\n",
    "    model=cv.dnn.readNet('yolov3.weights','yolov3.cfg')\n",
    "    layer_names=model.getLayerNames()\n",
    "    out_layers=[layer_names[i-1] for i in model.getUnconnectedOutLayers()]\n",
    "    \n",
    "    return model,out_layers,class_names\n",
    "\n",
    "def yolo_detect(img,yolo_model,out_layers):\n",
    "    height,width=img.shape[0],img.shape[1]\n",
    "    test_img=cv.dnn.blobFromImage(img,1.0/256,(448,448),(0,0,0),swapRB=True)\n",
    "    \n",
    "    yolo_model.setInput(test_img)\n",
    "    output3=yolo_model.forward(out_layers)\n",
    "    \n",
    "    box,conf,id=[],[],[]\t\t# 박스, 신뢰도, 부류 번호\n",
    "    for output in output3:\n",
    "        for vec85 in output:\n",
    "            scores=vec85[5:]\n",
    "            class_id=np.argmax(scores)\n",
    "            confidence=scores[class_id]\n",
    "            if confidence>0.5:\t# 신뢰도가 50% 이상인 경우만 취함\n",
    "                centerx,centery=int(vec85[0]*width),int(vec85[1]*height)\n",
    "                w,h=int(vec85[2]*width),int(vec85[3]*height)\n",
    "                x,y=int(centerx-w/2),int(centery-h/2)\n",
    "                box.append([x,y,x+w,y+h])\n",
    "                conf.append(float(confidence))\n",
    "                id.append(class_id)\n",
    "            \n",
    "    ind=cv.dnn.NMSBoxes(box,conf,0.5,0.4)\n",
    "    objects=[box[i]+[conf[i]]+[id[i]] for i in range(len(box)) if i in ind]\n",
    "    return objects\n",
    "\n",
    "model,out_layers,class_names=construct_yolo_v3()\t# YOLO 모델 생성\n",
    "colors=np.random.uniform(0,255,size=(100,3))\t\t# 100개 색으로 트랙 구분\n",
    "\n",
    "from sort import Sort\n",
    "\n",
    "sort=Sort()\n",
    "\n",
    "cap=cv.VideoCapture(0)\n",
    "if not cap.isOpened(): sys.exit('카메라 연결 실패')\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret: sys.exit('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        \n",
    "    res=yolo_detect(frame,model,out_layers)   \n",
    "    persons=[res[i] for i in range(len(res)) if res[i][5]==0] # 부류 0은 사람\n",
    "\n",
    "    if len(persons)==0: \n",
    "        tracks=sort.update()\n",
    "    else:\n",
    "        tracks=sort.update(np.array(persons))\n",
    "    \n",
    "    for i in range(len(tracks)):\n",
    "        x1,y1,x2,y2,track_id=tracks[i].astype(int)\n",
    "        cv.rectangle(frame,(x1,y1),(x2,y2),colors[track_id],2)\n",
    "        cv.putText(frame,str(track_id),(x1+10,y1+40),cv.FONT_HERSHEY_PLAIN,3,colors[track_id],2)            \n",
    "    \n",
    "    cv.imshow('Person tracking by SORT',frame)\n",
    "    \n",
    "    key=cv.waitKey(1) \n",
    "    if key==ord('q'): break \n",
    "    \n",
    "cap.release()\t\t# 카메라와 연결을 끊음\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 동영상 미리보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sys\n",
    "\n",
    "def construct_yolo_v3():\n",
    "    f=open('coco_names.txt', 'r')\n",
    "    class_names=[line.strip() for line in f.readlines()]\n",
    "\n",
    "    model=cv.dnn.readNet('yolov3.weights','yolov3.cfg')\n",
    "    layer_names=model.getLayerNames()\n",
    "    out_layers=[layer_names[i-1] for i in model.getUnconnectedOutLayers()]\n",
    "    \n",
    "    return model,out_layers,class_names\n",
    "\n",
    "def yolo_detect(img,yolo_model,out_layers):\n",
    "    height,width=img.shape[0],img.shape[1]\n",
    "    test_img=cv.dnn.blobFromImage(img,1.0/256,(448,448),(0,0,0),swapRB=True)\n",
    "    \n",
    "    yolo_model.setInput(test_img)\n",
    "    output3=yolo_model.forward(out_layers)\n",
    "    \n",
    "    box,conf,id=[],[],[]\t\t# 박스, 신뢰도, 부류 번호\n",
    "    for output in output3:\n",
    "        for vec85 in output:\n",
    "            scores=vec85[5:]\n",
    "            class_id=np.argmax(scores)\n",
    "            confidence=scores[class_id]\n",
    "            if confidence>0.5:\t# 신뢰도가 50% 이상인 경우만 취함\n",
    "                centerx,centery=int(vec85[0]*width),int(vec85[1]*height)\n",
    "                w,h=int(vec85[2]*width),int(vec85[3]*height)\n",
    "                x,y=int(centerx-w/2),int(centery-h/2)\n",
    "                box.append([x,y,x+w,y+h])\n",
    "                conf.append(float(confidence))\n",
    "                id.append(class_id)\n",
    "            \n",
    "    ind=cv.dnn.NMSBoxes(box,conf,0.5,0.4)\n",
    "    objects=[box[i]+[conf[i]]+[id[i]] for i in range(len(box)) if i in ind]\n",
    "    return objects\n",
    "\n",
    "model,out_layers,class_names=construct_yolo_v3()\t# YOLO 모델 생성\n",
    "colors=np.random.uniform(0,255,size=(100,3))\t\t# 100개 색으로 트랙 구분\n",
    "\n",
    "from sort import Sort\n",
    "sort=Sort()\n",
    "try: \n",
    "    #cap = cv.VideoCapture(0) # cam mode\n",
    "    cap = cv.VideoCapture(video_path) # video file mode\n",
    "    print(\"file\")\n",
    "except:\n",
    "    print(\"cam\")\n",
    "    cap=cv.VideoCapture(0,cv.CAP_DSHOW)\n",
    "\n",
    "\n",
    "if not cap.isOpened(): sys.exit('카메라 연결 실패')\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret: sys.exit('프레임 획득에 실패하여 루프를 나갑니다.')\n",
    "        \n",
    "    res=yolo_detect(frame,model,out_layers)   \n",
    "    persons=[res[i] for i in range(len(res)) if res[i][5]==0] # 부류 0은 사람\n",
    "\n",
    "    if len(persons)==0: \n",
    "        tracks=sort.update()\n",
    "    else:\n",
    "        tracks=sort.update(np.array(persons))\n",
    "    \n",
    "    for i in range(len(tracks)):\n",
    "        x1,y1,x2,y2,track_id=tracks[i].astype(int)\n",
    "        cv.rectangle(frame,(x1,y1),(x2,y2),colors[track_id],2)\n",
    "        cv.putText(frame,str(track_id),(x1+10,y1+40),cv.FONT_HERSHEY_PLAIN,3,colors[track_id],2)            \n",
    "    \n",
    "    cv.imshow('Person tracking by SORT',frame)\n",
    "    \n",
    "    key=cv.waitKey(1) \n",
    "    if key==ord('q'): break \n",
    "    \n",
    "cap.release()\t\t# 카메라와 연결을 끊음\n",
    "cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# txt 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sys\n",
    "\n",
    "def construct_yolo_v3():\n",
    "    f=open('coco_names.txt', 'r')\n",
    "    class_names=[line.strip() for line in f.readlines()]\n",
    "\n",
    "    model=cv.dnn.readNet('yolov3.weights','yolov3.cfg')\n",
    "    layer_names=model.getLayerNames()\n",
    "    out_layers=[layer_names[i-1] for i in model.getUnconnectedOutLayers()]\n",
    "    \n",
    "    return model,out_layers,class_names\n",
    "\n",
    "def yolo_detect(img,yolo_model,out_layers):\n",
    "    height,width=img.shape[0],img.shape[1]\n",
    "    test_img=cv.dnn.blobFromImage(img,1.0/256,(448,448),(0,0,0),swapRB=True)\n",
    "    \n",
    "    yolo_model.setInput(test_img)\n",
    "    output3=yolo_model.forward(out_layers)\n",
    "    \n",
    "    box,conf,id=[],[],[]\t\t# 박스, 신뢰도, 부류 번호\n",
    "    for output in output3:\n",
    "        for vec85 in output:\n",
    "            scores=vec85[5:]\n",
    "            class_id=np.argmax(scores)\n",
    "            confidence=scores[class_id]\n",
    "            if confidence>0.5:\t# 신뢰도가 50% 이상인 경우만 취함\n",
    "                centerx,centery=int(vec85[0]*width),int(vec85[1]*height)\n",
    "                w,h=int(vec85[2]*width),int(vec85[3]*height)\n",
    "                x,y=int(centerx-w/2),int(centery-h/2)\n",
    "                box.append([x,y,x+w,y+h])\n",
    "                conf.append(float(confidence))\n",
    "                id.append(class_id)\n",
    "            \n",
    "    ind=cv.dnn.NMSBoxes(box,conf,0.5,0.4)\n",
    "    objects=[box[i]+[conf[i]]+[id[i]] for i in range(len(box)) if i in ind]\n",
    "    return objects\n",
    "\n",
    "model,out_layers,class_names=construct_yolo_v3()\t# YOLO 모델 생성\n",
    "colors=np.random.uniform(0,255,size=(100,3))\t\t# 100개 색으로 트랙 구분\n",
    "\n",
    "model, out_layers, class_names = construct_yolo_v3()\n",
    "colors = np.random.uniform(0, 255, size=(100, 3))\n",
    "\n",
    "from sort import Sort\n",
    "sort = Sort()\n",
    "cap = cv.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened(): \n",
    "    sys.exit('동영상 파일을 열 수 없습니다.')\n",
    "\n",
    "# 결과를 저장할 파일 열기\n",
    "output_file = open('tracking_results.txt', 'w')\n",
    "\n",
    "frame_number = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: \n",
    "        break\n",
    "    \n",
    "    res = yolo_detect(frame, model, out_layers)   \n",
    "    persons = [res[i] for i in range(len(res)) if res[i][5] == 0]\n",
    "\n",
    "    if len(persons) == 0: \n",
    "        tracks = sort.update()\n",
    "    else:\n",
    "        tracks = sort.update(np.array(persons))\n",
    "    \n",
    "    for track in tracks:\n",
    "        x1, y1, x2, y2, track_id = track.astype(int)\n",
    "        # 파일에 프레임 번호와 바운딩 박스 정보 기록\n",
    "        output_file.write(f'Frame {frame_number}: ID {track_id} - Box [{x1}, {y1}, {x2}, {y2}]\\n')\n",
    "\n",
    "    frame_number += 1\n",
    "\n",
    "    key = cv.waitKey(1) \n",
    "    if key == ord('q'): \n",
    "        break \n",
    "\n",
    "cap.release()\n",
    "output_file.close()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2 as cv\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "def construct_yolo_v3():\n",
    "    f = open('coco_names.txt', 'r')\n",
    "    class_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    model = cv.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "    layer_names = model.getLayerNames()\n",
    "    out_layers = [layer_names[i - 1] for i in model.getUnconnectedOutLayers()]\n",
    "    \n",
    "    return model, out_layers, class_names\n",
    "\n",
    "def yolo_detect(img, yolo_model, out_layers):\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    test_img = cv.dnn.blobFromImage(img, 1.0 / 256, (448, 448), (0, 0, 0), swapRB=True)\n",
    "    \n",
    "    yolo_model.setInput(test_img)\n",
    "    output3 = yolo_model.forward(out_layers)\n",
    "    \n",
    "    box, conf, id = [], [], []  # 박스, 신뢰도, 부류 번호\n",
    "    for output in output3:\n",
    "        for vec85 in output:\n",
    "            scores = vec85[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:  # 신뢰도가 50% 이상인 경우만 취함\n",
    "                centerx, centery = int(vec85[0] * width), int(vec85[1] * height)\n",
    "                w, h = int(vec85[2] * width), int(vec85[3] * height)\n",
    "                x, y = int(centerx - w / 2), int(centery - h / 2)\n",
    "                box.append([x, y, x + w, y + h])\n",
    "                conf.append(float(confidence))\n",
    "                id.append(class_id)\n",
    "            \n",
    "    ind = cv.dnn.NMSBoxes(box, conf, 0.5, 0.4)\n",
    "    objects = [box[i] + [conf[i]] + [id[i]] for i in range(len(box)) if i in ind]\n",
    "    return objects\n",
    "\n",
    "model, out_layers, class_names = construct_yolo_v3()\n",
    "colors = np.random.uniform(0, 255, size=(100, 3))\n",
    "\n",
    "from sort import Sort\n",
    "sort = Sort()\n",
    "cap = cv.VideoCapture(video_path)\n",
    "\n",
    "if not cap.isOpened(): \n",
    "    sys.exit('동영상 파일을 열 수 없습니다.')\n",
    "\n",
    "# 결과를 저장할 리스트 생성\n",
    "tracking_results = []\n",
    "\n",
    "frame_number = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: \n",
    "        break\n",
    "    \n",
    "    res = yolo_detect(frame, model, out_layers)   \n",
    "    persons = [res[i] for i in range(len(res)) if res[i][5] == 0]\n",
    "\n",
    "    if len(persons) == 0: \n",
    "        tracks = sort.update()\n",
    "    else:\n",
    "        tracks = sort.update(np.array(persons))\n",
    "    \n",
    "    for track in tracks:\n",
    "        x1, y1, x2, y2, track_id = track.astype(int)\n",
    "        # 리스트에 결과 추가\n",
    "        tracking_results.append({'Frame': frame_number, 'ID': track_id, 'Box': [x1, y1, x2, y2]})\n",
    "\n",
    "    frame_number += 1\n",
    "\n",
    "    key = cv.waitKey(1) \n",
    "    if key == ord('q'): \n",
    "        break \n",
    "\n",
    "cap.release()\n",
    "cv.destroyAllWindows()\n",
    "\n",
    "# 결과를 DataFrame으로 변환하고 CSV 파일로 저장\n",
    "tracking_results_df = pd.DataFrame(tracking_results)\n",
    "tracking_results_df.to_csv('tracking_results_mevid.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 동영상 + 텍스트 결과 보기 (csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_tracking_data(file_path):\n",
    "    tracking_df = pd.read_csv(file_path)\n",
    "    tracking_data = {}\n",
    "\n",
    "    for _, row in tracking_df.iterrows():\n",
    "        frame_number = row['Frame']\n",
    "        track_id = row['ID']\n",
    "        box = eval(row['Box'])  # 문자열 형태의 리스트를 실제 리스트 객체로 변환\n",
    "\n",
    "        if frame_number not in tracking_data:\n",
    "            tracking_data[frame_number] = []\n",
    "        tracking_data[frame_number].append((track_id, box))\n",
    "    \n",
    "    return tracking_data\n",
    "\n",
    "\n",
    "# Load tracking data\n",
    "tracking_data_path = 'tracking_results_mevid.csv'\n",
    "tracking_data = load_tracking_data(tracking_data_path)\n",
    "\n",
    "cap = cv.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    sys.exit('동영상 파일을 열 수 없습니다.')\n",
    "\n",
    "# Define the codec and create VideoWriter object\n",
    "#mode = 'avi'\n",
    "mode = 'mp4'\n",
    "if mode == 'avi':\n",
    "    fourcc = cv.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv.VideoWriter('output.avi', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "elif mode == 'mp4':\n",
    "    fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv.VideoWriter('output2.mp4', fourcc, 20.0, (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "frame_number = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if frame_number in tracking_data:\n",
    "        for track_id, box in tracking_data[frame_number]:\n",
    "            x1, y1, x2, y2 = box\n",
    "            cv.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv.putText(frame, str(track_id), (x1, y1 - 10), cv.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    # Write the frame into the file 'output.avi'\n",
    "    out.write(frame)\n",
    "\n",
    "    cv.imshow('Tracking', frame)\n",
    "    frame_number += 1\n",
    "\n",
    "    if cv.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release everything if job is finished\n",
    "cap.release()\n",
    "out.release()\n",
    "cv.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['track_2.avi',\n",
       " 'track_1.avi',\n",
       " 'track_3.avi',\n",
       " 'track_5.avi',\n",
       " 'track_6.avi',\n",
       " 'track_7.avi',\n",
       " 'track_10.avi',\n",
       " 'track_12.avi',\n",
       " 'track_11.avi',\n",
       " 'track_13.avi',\n",
       " 'track_16.avi',\n",
       " 'track_17.avi',\n",
       " 'track_18.avi',\n",
       " 'track_19.avi',\n",
       " 'track_20.avi',\n",
       " 'track_23.avi',\n",
       " 'track_24.avi',\n",
       " 'track_26.avi',\n",
       " 'track_27.avi',\n",
       " 'track_28.avi',\n",
       " 'track_30.avi',\n",
       " 'track_31.avi',\n",
       " 'track_32.avi',\n",
       " 'track_33.avi',\n",
       " 'track_34.avi',\n",
       " 'track_35.avi',\n",
       " 'track_36.avi',\n",
       " 'track_37.avi',\n",
       " 'track_38.avi',\n",
       " 'track_39.avi',\n",
       " 'track_41.avi',\n",
       " 'track_43.avi',\n",
       " 'track_46.avi',\n",
       " 'track_48.avi',\n",
       " 'track_49.avi',\n",
       " 'track_50.avi',\n",
       " 'track_51.avi',\n",
       " 'track_52.avi',\n",
       " 'track_53.avi',\n",
       " 'track_54.avi',\n",
       " 'track_58.avi',\n",
       " 'track_60.avi',\n",
       " 'track_62.avi',\n",
       " 'track_66.avi',\n",
       " 'track_67.avi',\n",
       " 'track_68.avi',\n",
       " 'track_69.avi',\n",
       " 'track_70.avi',\n",
       " 'track_74.avi',\n",
       " 'track_76.avi',\n",
       " 'track_75.avi',\n",
       " 'track_77.avi',\n",
       " 'track_78.avi',\n",
       " 'track_80.avi',\n",
       " 'track_79.avi',\n",
       " 'track_81.avi',\n",
       " 'track_82.avi',\n",
       " 'track_87.avi',\n",
       " 'track_88.avi',\n",
       " 'track_92.avi',\n",
       " 'track_91.avi',\n",
       " 'track_95.avi',\n",
       " 'track_94.avi',\n",
       " 'track_96.avi',\n",
       " 'track_99.avi',\n",
       " 'track_101.avi',\n",
       " 'track_102.avi',\n",
       " 'track_103.avi',\n",
       " 'track_106.avi',\n",
       " 'track_108.avi',\n",
       " 'track_107.avi',\n",
       " 'track_110.avi',\n",
       " 'track_113.avi',\n",
       " 'track_114.avi',\n",
       " 'track_121.avi',\n",
       " 'track_120.avi',\n",
       " 'track_126.avi',\n",
       " 'track_125.avi',\n",
       " 'track_124.avi',\n",
       " 'track_127.avi',\n",
       " 'track_129.avi',\n",
       " 'track_130.avi',\n",
       " 'track_131.avi',\n",
       " 'track_135.avi',\n",
       " 'track_134.avi',\n",
       " 'track_138.avi',\n",
       " 'track_139.avi',\n",
       " 'track_140.avi',\n",
       " 'track_141.avi',\n",
       " 'track_142.avi',\n",
       " 'track_144.avi',\n",
       " 'track_145.avi',\n",
       " 'track_147.avi',\n",
       " 'track_151.avi',\n",
       " 'track_154.avi',\n",
       " 'track_153.avi',\n",
       " 'track_155.avi',\n",
       " 'track_157.avi',\n",
       " 'track_158.avi',\n",
       " 'track_159.avi',\n",
       " 'track_160.avi',\n",
       " 'track_161.avi',\n",
       " 'track_162.avi',\n",
       " 'track_167.avi',\n",
       " 'track_170.avi',\n",
       " 'track_171.avi',\n",
       " 'track_172.avi',\n",
       " 'track_173.avi',\n",
       " 'track_174.avi',\n",
       " 'track_176.avi',\n",
       " 'track_181.avi',\n",
       " 'track_183.avi',\n",
       " 'track_182.avi',\n",
       " 'track_184.avi',\n",
       " 'track_185.avi',\n",
       " 'track_187.avi',\n",
       " 'track_186.avi',\n",
       " 'track_191.avi',\n",
       " 'track_193.avi',\n",
       " 'track_194.avi',\n",
       " 'track_196.avi',\n",
       " 'track_198.avi',\n",
       " 'track_200.avi',\n",
       " 'track_201.avi',\n",
       " 'track_202.avi',\n",
       " 'track_203.avi',\n",
       " 'track_205.avi',\n",
       " 'track_207.avi',\n",
       " 'track_211.avi',\n",
       " 'track_217.avi',\n",
       " 'track_216.avi',\n",
       " 'track_220.avi',\n",
       " 'track_221.avi',\n",
       " 'track_222.avi',\n",
       " 'track_223.avi',\n",
       " 'track_225.avi',\n",
       " 'track_226.avi',\n",
       " 'track_228.avi',\n",
       " 'track_227.avi',\n",
       " 'track_230.avi',\n",
       " 'track_231.avi',\n",
       " 'track_232.avi',\n",
       " 'track_233.avi',\n",
       " 'track_234.avi',\n",
       " 'track_238.avi',\n",
       " 'track_239.avi',\n",
       " 'track_240.avi',\n",
       " 'track_241.avi',\n",
       " 'track_244.avi',\n",
       " 'track_245.avi',\n",
       " 'track_246.avi',\n",
       " 'track_247.avi',\n",
       " 'track_248.avi',\n",
       " 'track_251.avi',\n",
       " 'track_252.avi',\n",
       " 'track_253.avi',\n",
       " 'track_255.avi',\n",
       " 'track_256.avi',\n",
       " 'track_257.avi',\n",
       " 'track_258.avi',\n",
       " 'track_259.avi',\n",
       " 'track_261.avi',\n",
       " 'track_263.avi',\n",
       " 'track_264.avi',\n",
       " 'track_267.avi',\n",
       " 'track_269.avi',\n",
       " 'track_271.avi',\n",
       " 'track_272.avi',\n",
       " 'track_273.avi',\n",
       " 'track_276.avi',\n",
       " 'track_275.avi']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to parse a line from the tracking results and extract frame number, track ID, and bounding box coordinates\n",
    "def parse_tracking_line(line):\n",
    "    match = re.match(r'Frame (\\d+): ID (\\d+) - Box \\[(\\d+), (\\d+), (\\d+), (\\d+)\\]', line)\n",
    "    if match:\n",
    "        frame, track_id, x1, y1, x2, y2 = map(int, match.groups())\n",
    "        return frame, track_id, (x1, y1, x2, y2)\n",
    "    return None\n",
    "\n",
    "def parse_tracking_line(line):  \n",
    "    # Splitting the string to extract the relevant parts\n",
    "    parts = line.split()\n",
    "\n",
    "    # Extracting frame and track_id\n",
    "    frame = int(parts[1].rstrip(':'))\n",
    "    track_id = int(parts[3])\n",
    "\n",
    "    # Extracting coordinates\n",
    "    coordinates_str = input_str.split('[')[1].split(']')[0]\n",
    "    coordinates = tuple(map(int, coordinates_str.split(',')))\n",
    "\n",
    "    # Returning the extracted information\n",
    "    return frame, track_id, coordinates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############## fix\n",
    "\n",
    "def extract_info_from_file(file_path):\n",
    "\n",
    "    def extract_info(input_str):\n",
    "        # Splitting the string to extract the relevant parts\n",
    "        parts = input_str.split()\n",
    "\n",
    "        # Extracting frame and track_id\n",
    "        frame = int(parts[1].rstrip(':'))\n",
    "        track_id = int(parts[3])\n",
    "\n",
    "        # Extracting coordinates\n",
    "        coordinates_str = input_str.split('[')[1].split(']')[0]\n",
    "        coordinates = tuple(map(int, coordinates_str.split(',')))\n",
    "\n",
    "        # Returning the extracted information\n",
    "        return frame, track_id, coordinates\n",
    "\n",
    "    # Reading the file and processing each line\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        return [extract_info(line.strip()) for line in lines]\n",
    "\n",
    "# Path to the uploaded file\n",
    "file_path = 'tracking_results.txt'\n",
    "\n",
    "# Extracting information from the file\n",
    "\n",
    "\n",
    "\n",
    "##############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parsing the tracking data\n",
    "#tracking_info = parse_tracking_line('tracking_results.txt')\n",
    "\n",
    "\n",
    "#####\n",
    "extracted_data = extract_info_from_file(file_path)\n",
    "tracking_info = extracted_data\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "# Opening the video file\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not video.isOpened():\n",
    "    raise IOError(\"Error opening video file\")\n",
    "\n",
    "# Video parameters\n",
    "frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Dictionary to store video writers for each track ID\n",
    "output_videos = defaultdict(lambda: None)\n",
    "\n",
    "# Process video frame by frame\n",
    "current_frame = 0\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process tracking data for the current frame\n",
    "    frame_tracks = [info for info in tracking_info if info[0] == current_frame]\n",
    "    for _, track_id, (x1, y1, x2, y2) in frame_tracks:\n",
    "        # Convert to square bounding box\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        side_length = max(width, height)\n",
    "        center_x, center_y = x1 + width // 2, y1 + height // 2\n",
    "        new_x1 = max(center_x - side_length // 2, 0)\n",
    "        new_y1 = max(center_y - side_length // 2, 0)\n",
    "        new_x2 = min(center_x + side_length // 2, frame_width)\n",
    "        new_y2 = min(center_y + side_length // 2, frame_height)\n",
    "\n",
    "        # Extract the bounding box\n",
    "        bbox_frame = frame[new_y1:new_y2, new_x1:new_x2]\n",
    "\n",
    "        # Check if video writer is already created for this track ID\n",
    "        if output_videos[track_id] is None:\n",
    "            output_path = f'/mnt/data/track_{track_id}.avi'\n",
    "            output_videos[track_id] = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), frame_rate, (side_length, side_length))\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        output_videos[track_id].write(bbox_frame)\n",
    "\n",
    "    current_frame += 1\n",
    "\n",
    "# Release resources\n",
    "video.release()\n",
    "for writer in output_videos.values():\n",
    "    if writer:\n",
    "        writer.release()\n",
    "\n",
    "# Provide paths for the user to download\n",
    "output_video_files = [f'track_{track_id}.avi' for track_id in output_videos.keys()]\n",
    "output_video_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to parse a line from the tracking results and extract frame number, track ID, and bounding box coordinates\n",
    "def parse_tracking_line(line):\n",
    "    match = re.match(r'Frame (\\d+): ID (\\d+) - Box \\[(\\d+), (\\d+), (\\d+), (\\d+)\\]', line)\n",
    "    if match:\n",
    "        frame, track_id, x1, y1, x2, y2 = map(int, match.groups())\n",
    "        return frame, track_id, (x1, y1, x2, y2)\n",
    "    return None\n",
    "\n",
    "def parse_tracking_line(line):  \n",
    "    # Splitting the string to extract the relevant parts\n",
    "    parts = line.split()\n",
    "\n",
    "    # Extracting frame and track_id\n",
    "    frame = int(parts[1].rstrip(':'))\n",
    "    track_id = int(parts[3])\n",
    "\n",
    "    # Extracting coordinates\n",
    "    coordinates_str = input_str.split('[')[1].split(']')[0]\n",
    "    coordinates = tuple(map(int, coordinates_str.split(',')))\n",
    "\n",
    "    # Returning the extracted information\n",
    "    return frame, track_id, coordinates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############## fix\n",
    "\n",
    "def extract_info_from_file(file_path):\n",
    "\n",
    "    def extract_info(input_str):\n",
    "        # Splitting the string to extract the relevant parts\n",
    "        parts = input_str.split()\n",
    "\n",
    "        # Extracting frame and track_id\n",
    "        frame = int(parts[1].rstrip(':'))\n",
    "        track_id = int(parts[3])\n",
    "\n",
    "        # Extracting coordinates\n",
    "        coordinates_str = input_str.split('[')[1].split(']')[0]\n",
    "        coordinates = tuple(map(int, coordinates_str.split(',')))\n",
    "\n",
    "        # Returning the extracted information\n",
    "        return frame, track_id, coordinates\n",
    "\n",
    "    # Reading the file and processing each line\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        return [extract_info(line.strip()) for line in lines]\n",
    "\n",
    "# Path to the uploaded file\n",
    "file_path = 'tracking_results.txt'\n",
    "\n",
    "# Extracting information from the file\n",
    "\n",
    "\n",
    "\n",
    "##############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Parsing the tracking data\n",
    "#tracking_info = parse_tracking_line('tracking_results.txt')\n",
    "\n",
    "\n",
    "#####\n",
    "extracted_data = extract_info_from_file(file_path)\n",
    "tracking_info = extracted_data\n",
    "\n",
    "####\n",
    "\n",
    "\n",
    "# Opening the video file\n",
    "video = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not video.isOpened():\n",
    "    raise IOError(\"Error opening video file\")\n",
    "\n",
    "# Video parameters\n",
    "frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frame_rate = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Dictionary to store video writers for each track ID\n",
    "output_videos = defaultdict(lambda: None)\n",
    "\n",
    "# Process video frame by frame\n",
    "current_frame = 0\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Process tracking data for the current frame\n",
    "    frame_tracks = [info for info in tracking_info if info[0] == current_frame]\n",
    "    for _, track_id, (x1, y1, x2, y2) in frame_tracks:\n",
    "        # Convert to square bounding box\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        side_length = max(width, height)\n",
    "        center_x, center_y = x1 + width // 2, y1 + height // 2\n",
    "        new_x1 = max(center_x - side_length // 2, 0)\n",
    "        new_y1 = max(center_y - side_length // 2, 0)\n",
    "        new_x2 = min(center_x + side_length // 2, frame_width)\n",
    "        new_y2 = min(center_y + side_length // 2, frame_height)\n",
    "\n",
    "        # Extract the bounding box\n",
    "        bbox_frame = frame[new_y1:new_y2, new_x1:new_x2]\n",
    "\n",
    "        # Check if video writer is already created for this track ID\n",
    "        if output_videos[track_id] is None:\n",
    "            output_path = f'/mnt/data/track_{track_id}.avi'\n",
    "            output_videos[track_id] = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), frame_rate, (side_length, side_length))\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        output_videos[track_id].write(bbox_frame)\n",
    "\n",
    "    current_frame += 1\n",
    "\n",
    "# Release resources\n",
    "video.release()\n",
    "for writer in output_videos.values():\n",
    "    if writer:\n",
    "        writer.release()\n",
    "\n",
    "# Provide paths for the user to download\n",
    "output_video_files = [f'track_{track_id}.avi' for track_id in output_videos.keys()]\n",
    "output_video_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Assuming output_videos is a dictionary with track_id as keys and video frames as values\n",
    "output_videos = {}  # Populate this with your actual video data\n",
    "\n",
    "# Directory where videos will be saved\n",
    "output_directory = 'path/to/your/output/folder'  # Replace with the actual path\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Function to save each video\n",
    "def save_video(track_id, frames, output_dir):\n",
    "    # Define the path for the output video\n",
    "    output_path = os.path.join(output_dir, f'track_{track_id}.avi')\n",
    "\n",
    "    # Define video writer\n",
    "    # Assuming all frames have the same size and type, use the properties of the first frame\n",
    "    height, width, layers = frames[0].shape\n",
    "    video_writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'DIVX'), 30, (width, height))\n",
    "\n",
    "    # Write frames to the video\n",
    "    for frame in frames:\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    # Release the video writer\n",
    "    video_writer.release()\n",
    "\n",
    "# Iterate over the output videos and save them\n",
    "for track_id, frames in output_videos.items():\n",
    "    save_video(track_id, frames, output_directory)\n",
    "\n",
    "# Provide paths for the user to download\n",
    "output_video_files = [os.path.join(output_directory, f'track_{track_id}.avi') for track_id in output_videos.keys()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb 셀 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     video_writer\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb#X20sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39m# Iterate over the output videos and save them\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mfor\u001b[39;00m track_id, frames \u001b[39min\u001b[39;00m output_videos\u001b[39m.\u001b[39;49mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     save_video(track_id, frames, output_directory)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb#X20sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Provide paths for the user to download\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Assuming output_videos is a dictionary with track_id as keys and video frames as values\n",
    "output_videos = output_video_files  # Populate this with your actual video data\n",
    "\n",
    "# Directory where videos will be saved\n",
    "output_directory = 'ouput'  # Replace with the actual path\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "\n",
    "# Function to save each video\n",
    "def save_video(track_id, frames, output_dir):\n",
    "    # Define the path for the output video\n",
    "    output_path = os.path.join(output_dir, f'track_{track_id}.avi')\n",
    "\n",
    "    # Define video writer\n",
    "    # Assuming all frames have the same size and type, use the properties of the first frame\n",
    "    height, width, layers = frames[0].shape\n",
    "    video_writer = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'DIVX'), 30, (width, height))\n",
    "\n",
    "    # Write frames to the video\n",
    "    for frame in frames:\n",
    "        video_writer.write(frame)\n",
    "\n",
    "    # Release the video writer\n",
    "    video_writer.release()\n",
    "\n",
    "# Iterate over the output videos and save them\n",
    "for track_id, frames in output_videos.items():\n",
    "    save_video(track_id, frames, output_directory)\n",
    "\n",
    "# Provide paths for the user to download\n",
    "output_video_files = [os.path.join(output_directory, f'track_{track_id}.avi') for track_id in output_videos.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crop video csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping complete. Videos saved in cropped_videos\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_tracking_data(csv_path):\n",
    "    tracking_df = pd.read_csv(csv_path)\n",
    "    tracking_data = {}\n",
    "    for _, row in tracking_df.iterrows():\n",
    "        frame = row['Frame']\n",
    "        track_id = row['ID']\n",
    "        box = eval(row['Box'])\n",
    "        if frame not in tracking_data:\n",
    "            tracking_data[frame] = []\n",
    "        tracking_data[frame].append((track_id, box))\n",
    "    return tracking_data\n",
    "\n",
    "def save_cropped_video(video_path, tracking_data, output_dir):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Error opening video file\")\n",
    "    \n",
    "    frame_number = 0\n",
    "    video_writers = {}\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_number in tracking_data:\n",
    "            for track_id, (x1, y1, x2, y2) in tracking_data[frame_number]:\n",
    "                cropped_frame = frame[y1:y2, x1:x2]\n",
    "                ### save as avi\n",
    "                # if track_id not in video_writers: \n",
    "                #     frame_height, frame_width = cropped_frame.shape[:2]\n",
    "                #     output_path = os.path.join(output_dir, f'track_{track_id}.avi')\n",
    "                #     video_writers[track_id] = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'XVID'), cap.get(cv2.CAP_PROP_FPS), (frame_width, frame_height))\n",
    "                if track_id not in video_writers:\n",
    "                    frame_height, frame_width = cropped_frame.shape[:2]\n",
    "                    output_path = os.path.join(output_dir, f'track_{track_id}.mp4')\n",
    "                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Change here for MP4\n",
    "                    video_writers[track_id] = cv2.VideoWriter(output_path, fourcc, cap.get(cv2.CAP_PROP_FPS), (frame_width, frame_height))\n",
    "\n",
    "                video_writers[track_id].write(cropped_frame)\n",
    "\n",
    "                video_writers[track_id].write(cropped_frame)\n",
    "\n",
    "        frame_number += 1\n",
    "\n",
    "    cap.release()\n",
    "    for writer in video_writers.values():\n",
    "        writer.release()\n",
    "\n",
    "# Paths\n",
    "csv_path = 'tracking_results.csv'\n",
    "output_dir = 'cropped_videos'\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process the video\n",
    "tracking_data = load_tracking_data(csv_path)\n",
    "save_cropped_video(video_path, tracking_data, output_dir)\n",
    "\n",
    "print(\"Cropping complete. Videos saved in\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crop video again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping complete. Videos saved in cropped_videos\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_tracking_data(csv_path):\n",
    "    tracking_df = pd.read_csv(csv_path)\n",
    "    tracking_data = {}\n",
    "    for _, row in tracking_df.iterrows():\n",
    "        frame = row['Frame']\n",
    "        track_id = row['ID']\n",
    "        box = eval(row['Box'])  # Convert string representation of list to actual list\n",
    "        if frame not in tracking_data:\n",
    "            tracking_data[frame] = []\n",
    "        tracking_data[frame].append((track_id, box))\n",
    "    return tracking_data\n",
    "\n",
    "def save_cropped_videos(video_path, tracking_data, output_dir):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Error opening video file\")\n",
    "    \n",
    "    frame_number = 0\n",
    "    video_writers = {}\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if frame_number in tracking_data:\n",
    "            for track_id, (x1, y1, x2, y2) in tracking_data[frame_number]:\n",
    "                cropped_frame = frame[y1:y2, x1:x2]\n",
    "\n",
    "                if track_id not in video_writers:\n",
    "                    height, width = cropped_frame.shape[:2]\n",
    "                    output_path = os.path.join(output_dir, f'track_{track_id}.mp4')\n",
    "                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # MP4 codec\n",
    "                    video_writers[track_id] = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "                video_writers[track_id].write(cropped_frame)\n",
    "\n",
    "        frame_number += 1\n",
    "\n",
    "    cap.release()\n",
    "    for writer in video_writers.values():\n",
    "        writer.release()\n",
    "\n",
    "# Paths\n",
    "csv_path = 'tracking_results.csv'\n",
    "output_dir = 'cropped_videos'\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load tracking data and process the video\n",
    "tracking_data = load_tracking_data(csv_path)\n",
    "save_cropped_videos(video_path, tracking_data, output_dir)\n",
    "\n",
    "print(\"Cropping complete. Videos saved in\", output_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# re re video crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropping complete. Videos saved in output_videos\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_tracking_data(csv_path):\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "def crop_square(frame, box):\n",
    "    x, y, x2, y2 = box\n",
    "    height = y2 - y\n",
    "    center_x = x + (x2 - x) // 2\n",
    "    x1 = max(center_x - height // 2, 0)\n",
    "    x2 = min(center_x + height // 2, frame.shape[1])\n",
    "    return frame[y:y2, x1:x2]\n",
    "\n",
    "def save_cropped_videos(video_path, tracking_df, output_dir, fps=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise IOError(\"Error opening video file\")\n",
    "    \n",
    "    video_writers = {}\n",
    "    for _, row in tracking_df.iterrows():\n",
    "        frame_number = row['Frame']\n",
    "        track_id = row['ID']\n",
    "        box = eval(row['Box'])\n",
    "        \n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            continue\n",
    "        \n",
    "        cropped_frame = crop_square(frame, box)\n",
    "        height, width = cropped_frame.shape[:2]\n",
    "\n",
    "        if track_id not in video_writers:\n",
    "            output_path = os.path.join(output_dir, f'track_{track_id}.mp4')\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            video_writers[track_id] = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "        video_writers[track_id].write(cropped_frame)\n",
    "\n",
    "    cap.release()\n",
    "    for writer in video_writers.values():\n",
    "        writer.release()\n",
    "\n",
    "# Paths\n",
    "\n",
    "csv_path = 'tracking_results.csv'  # Update this path\n",
    "output_dir = 'output_videos'  # Update this path\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load tracking data and process the video\n",
    "tracking_df = load_tracking_data(csv_path)\n",
    "save_cropped_videos(video_path, tracking_df, output_dir)\n",
    "\n",
    "print(\"Cropping complete. Videos saved in\", output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crop by image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Define the function to extract and save frames based on tracking results\n",
    "def extract_frames(video_path, tracking_data, output_folder):\n",
    "    # Read the video file\n",
    "    video = VideoFileClip(video_path)\n",
    "    frame_rate = video.fps\n",
    "\n",
    "    for index, row in tracking_data.iterrows():\n",
    "        frame_number = row['Frame']\n",
    "        box = eval(row['Box'])  # Convert string representation of list to actual list\n",
    "        x1, y1, x2, y2 = box\n",
    "\n",
    "        # Calculate height and width of the box\n",
    "        height = y2 - y1\n",
    "        width = x2 - x1\n",
    "\n",
    "        # Calculate the size of the square to crop\n",
    "        size = max(width, height)\n",
    "\n",
    "        # Center the box\n",
    "        x_center = x1 + width // 2\n",
    "        y_center = y1 + height // 2\n",
    "\n",
    "        # Calculate new corners of the square box\n",
    "        new_x1 = max(x_center - size // 2, 0)\n",
    "        new_y1 = max(y_center - size // 2, 0)\n",
    "        new_x2 = min(x_center + size // 2, video.size[0])\n",
    "        new_y2 = min(y_center + size // 2, video.size[1])\n",
    "\n",
    "        # Extract the frame at the specified time\n",
    "        time = frame_number / frame_rate\n",
    "        frame = video.get_frame(time)\n",
    "\n",
    "        # Crop the frame to the square\n",
    "        cropped_frame = frame[new_y1:new_y2, new_x1:new_x2]\n",
    "\n",
    "        # Save the cropped frame\n",
    "        output_filename = os.path.join(output_folder, f\"frame_{frame_number}.png\")\n",
    "        cv2.imwrite(output_filename, cv2.cvtColor(cropped_frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    video.close()\n",
    "\n",
    "# Define the paths\n",
    "import pandas as pd\n",
    "output_folder = 'cropped_frames'\n",
    "tracking_results = pd.read_csv('tracking_results.csv')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Extract and save frames\n",
    "extract_frames(video_path, tracking_results, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crop by image (cv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb 셀 27\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb#X40sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m os\u001b[39m.\u001b[39mmakedirs(output_folder, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb#X40sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39m# Extract and save frames\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb#X40sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m extract_frames(video_path, tracking_results, output_folder)\n",
      "\u001b[1;32m/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb 셀 27\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb#X40sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m new_y2 \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(new_y1 \u001b[39m+\u001b[39m size, video_height)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb#X40sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Set the video frame position\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb#X40sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m video\u001b[39m.\u001b[39;49mset(cv2\u001b[39m.\u001b[39;49mCAP_PROP_POS_FRAMES, frame_number)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb#X40sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Read the frame\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/owo/HOUSE/-ActionClass/02YOLO/yolosort.ipynb#X40sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m success, frame \u001b[39m=\u001b[39m video\u001b[39m.\u001b[39mread()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the function to extract and save frames based on tracking results\n",
    "def extract_frames(video_path, tracking_data, output_folder):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frame_rate = video.get(cv2.CAP_PROP_FPS)\n",
    "    video_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    video_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    for index, row in tracking_data.iterrows():\n",
    "        frame_number = row['Frame']\n",
    "        box = eval(row['Box'])  # Convert string representation of list to actual list\n",
    "        x1, y1, x2, y2 = box\n",
    "\n",
    "        # Calculate the maximum size of the square based on video dimensions\n",
    "        max_square_size = min(video_width, video_height)\n",
    "\n",
    "        # Calculate height and width of the box\n",
    "        height = y2 - y1\n",
    "        width = x2 - x1\n",
    "\n",
    "        # Calculate the size of the square to crop, ensuring it does not exceed video dimensions\n",
    "        size = min(max(width, height), max_square_size)\n",
    "\n",
    "        # Center the box\n",
    "        x_center = x1 + width // 2\n",
    "        y_center = y1 + height // 2\n",
    "\n",
    "        # Calculate new corners of the square box\n",
    "        # Adjust the position to ensure the square does not exceed the video frame\n",
    "        new_x1 = max(x_center - size // 2, 0)\n",
    "        new_y1 = max(y_center - size // 2, 0)\n",
    "        new_x2 = min(new_x1 + size, video_width)\n",
    "        new_y2 = min(new_y1 + size, video_height)\n",
    "\n",
    "\n",
    "\n",
    "        # Set the video frame position\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "        # Read the frame\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            continue  # Skip if the frame is not read successfully\n",
    "\n",
    "        # Crop the frame to the square\n",
    "        cropped_frame = frame[new_y1:new_y2, new_x1:new_x2]\n",
    "\n",
    "        # Save the cropped frame\n",
    "        output_filename = os.path.join(output_folder, f\"frame_{frame_number}.png\")\n",
    "        cv2.imwrite(output_filename, cropped_frame)\n",
    "\n",
    "    video.release()\n",
    "\n",
    "# Define the paths\n",
    "tracking_results = pd.read_csv('tracking_results.csv')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Extract and save frames\n",
    "extract_frames(video_path, tracking_results, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# re crop video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# Function to extract frames based on ID and create videos\n",
    "def extract_and_create_videos(video_path, tracking_data, output_folder):\n",
    "    video = VideoFileClip(video_path)\n",
    "    frame_rate = video.fps\n",
    "\n",
    "    # Create a dictionary to store video writers for each ID\n",
    "    video_writers = {}\n",
    "\n",
    "    for index, row in tracking_data.iterrows():\n",
    "        frame_number = row['Frame']\n",
    "        box = eval(row['Box'])\n",
    "        person_id = row['ID']\n",
    "        x1, y1, x2, y2 = box\n",
    "\n",
    "        # Calculate the maximum size of the square based on video dimensions\n",
    "        max_square_size = min(video_width, video_height)\n",
    "\n",
    "        # Calculate height and width of the box\n",
    "        height = y2 - y1\n",
    "        width = x2 - x1\n",
    "\n",
    "        # Calculate the size of the square to crop, ensuring it does not exceed video dimensions\n",
    "        size = min(max(width, height), max_square_size)\n",
    "\n",
    "        # Center the box\n",
    "        x_center = x1 + width // 2\n",
    "        y_center = y1 + height // 2\n",
    "\n",
    "        # Calculate new corners of the square box\n",
    "        # Adjust the position to ensure the square does not exceed the video frame\n",
    "        new_x1 = max(x_center - size // 2, 0)\n",
    "        new_y1 = max(y_center - size // 2, 0)\n",
    "        new_x2 = min(new_x1 + size, video_width)\n",
    "        new_y2 = min(new_y1 + size, video_height)\n",
    "\n",
    "\n",
    "        # Extract the frame at the specified time\n",
    "        time = frame_number / frame_rate\n",
    "        frame = video.get_frame(time)\n",
    "\n",
    "        # Crop the frame to the square\n",
    "        cropped_frame = frame[new_y1:new_y2, new_x1:new_x2]\n",
    "\n",
    "        # Check if the video writer for this ID exists\n",
    "        if person_id not in video_writers:\n",
    "            output_filename = os.path.join(output_folder, f\"video_id_{person_id}.mp4\")\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            video_writers[person_id] = cv2.VideoWriter(output_filename, fourcc, frame_rate, (size, size))\n",
    "\n",
    "        # Write the frame to the video\n",
    "        video_writers[person_id].write(cv2.cvtColor(cropped_frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Release all video writers\n",
    "    for writer in video_writers.values():\n",
    "        writer.release()\n",
    "\n",
    "    video.close()\n",
    "\n",
    "# Define the output folder for the videos\n",
    "output_folder = 'cropped_videos'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Extract frames and create videos\n",
    "extract_and_create_videos(video_path, tracking_results, output_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# again! 300x300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the function to extract and save frames based on tracking results\n",
    "def extract_frames(video_path, tracking_data, output_folder):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    frame_rate = video.get(cv2.CAP_PROP_FPS)\n",
    "    video_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    video_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    for index, row in tracking_data.iterrows():\n",
    "        frame_number = row['Frame']\n",
    "        box = eval(row['Box'])  # Convert string representation of list to actual list\n",
    "        x1, y1, x2, y2 = box\n",
    "\n",
    "        # Calculate the maximum size of the square based on video dimensions\n",
    "        max_square_size = min(video_width, video_height)\n",
    "\n",
    "        # Calculate height and width of the box\n",
    "        height = y2 - y1\n",
    "        width = x2 - x1\n",
    "\n",
    "        # Calculate the size of the square to crop, ensuring it does not exceed video dimensions\n",
    "        size = min(max(width, height), max_square_size)\n",
    "\n",
    "        # Center the box\n",
    "        x_center = x1 + width // 2\n",
    "        y_center = y1 + height // 2\n",
    "\n",
    "        # Calculate new corners of the square box\n",
    "        # Adjust the position to ensure the square does not exceed the video frame\n",
    "        new_x1 = max(x_center - size // 2, 0)\n",
    "        new_y1 = max(y_center - size // 2, 0)\n",
    "        new_x2 = min(new_x1 + size, video_width)\n",
    "        new_y2 = min(new_y1 + size, video_height)\n",
    "\n",
    "        # Set the video frame position\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "\n",
    "        # Read the frame\n",
    "        success, frame = video.read()\n",
    "        if not success:\n",
    "            continue  # Skip if the frame is not read successfully\n",
    "\n",
    "        # Crop the frame to the square\n",
    "        cropped_frame = frame[new_y1:new_y2, new_x1:new_x2]\n",
    "\n",
    "        # Resize the cropped frame to 300x300 pixels\n",
    "        resized_frame = cv2.resize(cropped_frame, (300, 300))\n",
    "\n",
    "        # Save the resized frame\n",
    "        output_filename = os.path.join(output_folder, f\"frame_{frame_number}.png\")\n",
    "        cv2.imwrite(output_filename, resized_frame)\n",
    "\n",
    "    video.release()\n",
    "\n",
    "# Define the paths\n",
    "output_folder = 'cropped_videos'\n",
    "video_path = '../data/slip.mp4'  # Replace with your video path\n",
    "tracking_results = pd.read_csv('tracking_results.csv')\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Extract and save frames\n",
    "extract_frames(video_path, tracking_results, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def extract_and_create_videos(video_path, tracking_data, output_folder):\n",
    "    video = VideoFileClip(video_path)\n",
    "    frame_rate = video.fps\n",
    "\n",
    "    # Create a dictionary to store video writers for each ID\n",
    "    video_writers = {}\n",
    "\n",
    "    for index, row in tracking_data.iterrows():\n",
    "        frame_number = row['Frame']\n",
    "        box = eval(row['Box'])\n",
    "        person_id = row['ID']\n",
    "        x1, y1, x2, y2 = box\n",
    "\n",
    "        # Calculate height and width of the box\n",
    "        height = y2 - y1\n",
    "\n",
    "        # Center the box\n",
    "        x_center = x1 + (x2 - x1) // 2\n",
    "        y_center = y1 + height // 2\n",
    "\n",
    "        # Calculate new corners of the square box\n",
    "        new_x1 = max(x_center - height // 2, 0)\n",
    "        new_y1 = max(y_center - height // 2, 0)\n",
    "        new_x2 = min(x_center + height // 2, video.size[0])\n",
    "        new_y2 = min(y_center + height // 2, video.size[1])\n",
    "\n",
    "        # Extract the frame at the specified time\n",
    "        time = frame_number / frame_rate\n",
    "        frame = video.get_frame(time)\n",
    "\n",
    "        # Crop the frame to the square\n",
    "        cropped_frame = frame[new_y1:new_y2, new_x1:new_x2]\n",
    "\n",
    "        # Resize cropped frame to 300x300\n",
    "        resized_frame = cv2.resize(cropped_frame, (300, 300))\n",
    "\n",
    "        # Check if the video writer for this ID exists\n",
    "        if person_id not in video_writers:\n",
    "            output_filename = os.path.join(output_folder, f\"video_id_{person_id}.mp4\")\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            video_writers[person_id] = cv2.VideoWriter(output_filename, fourcc, frame_rate, (300, 300))\n",
    "\n",
    "        # Write the frame to the video\n",
    "        video_writers[person_id].write(cv2.cvtColor(resized_frame, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # Release all video writers\n",
    "    for writer in video_writers.values():\n",
    "        writer.release()\n",
    "\n",
    "    video.close()\n",
    "\n",
    "# Define the output folder for the videos\n",
    "video_path = '../data/slip.mp4'  # Replace with your video path\n",
    "tracking_results = pd.read_csv('tracking_results.csv')\n",
    "output_folder = 'cropped_videos'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Extract frames and create videos\n",
    "extract_and_create_videos(video_path, tracking_results, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def extract_and_create_videos(video_path, tracking_data, output_folder):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise Exception(\"Error opening video file\")\n",
    "\n",
    "    # Get video properties\n",
    "    frame_rate = cap.get(cv2.CAP_PROP_FPS)\n",
    "    video_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    video_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create a dictionary to store video writers for each ID\n",
    "    video_writers = {}\n",
    "\n",
    "    # Process each entry in the tracking data\n",
    "    for index, row in tracking_data.iterrows():\n",
    "        frame_number = row['Frame']\n",
    "        box = eval(row['Box'])\n",
    "        person_id = row['ID']\n",
    "        x1, y1, x2, y2 = box\n",
    "\n",
    "        # Ensure coordinates are within bounds\n",
    "        x1, y1, x2, y2 = max(x1, 0), max(y1, 0), min(x2, video_width), min(y2, video_height)\n",
    "\n",
    "        # Set the video to the specific frame\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret or frame is None:\n",
    "            continue\n",
    "\n",
    "        # Crop and resize frame\n",
    "        cropped_frame = frame[y1:y2, x1:x2]\n",
    "        if cropped_frame.size == 0:\n",
    "            continue  # Skip if the cropped frame is empty\n",
    "        resized_frame = cv2.resize(cropped_frame, (300, 300))\n",
    "\n",
    "        # Check if the video writer for this ID exists\n",
    "        if person_id not in video_writers:\n",
    "            output_filename = os.path.join(output_folder, f\"video_id_{person_id}.mp4\")\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            video_writers[person_id] = cv2.VideoWriter(output_filename, fourcc, frame_rate, (300, 300))\n",
    "\n",
    "        # Write the frame to the video\n",
    "        video_writers[person_id].write(resized_frame)\n",
    "\n",
    "    # Release all video writers\n",
    "    for writer in video_writers.values():\n",
    "        writer.release()\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# Define the output folder for the videos\n",
    "video_path = 'mevid.mp4'  # Replace with your video path\n",
    "tracking_data = pd.read_csv('abc.csv')\n",
    "output_folder = 'cropped_videos'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Extract frames and create videos\n",
    "extract_and_create_videos(video_path, tracking_data, output_folder)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
