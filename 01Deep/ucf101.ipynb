{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install av\n",
    "# ! pip install ffmpeg-python\n",
    "# ! pip install opencv-python\n",
    "# ! pip install mediapipe\n",
    "# ! pip install pandas\n",
    "# ! pip install tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import tensor\n",
    "import torch.nn as nn\n",
    "from torch.nn import DataParallel\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0), True)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    multi_gpu = True\n",
    "else:\n",
    "    multi_gpu = False\n",
    "device , multi_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skeleton to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skeleton_csv(video_path ='', output_video_path ='', make_video=True, output_video_name='skeleton_video'):\n",
    "    try:\n",
    "        # Initialize MediaPipe Pose\n",
    "        mp_pose = mp.solutions.pose\n",
    "        mp_drawing = mp.solutions.drawing_utils\n",
    "        pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False, min_detection_confidence=0.5)\n",
    "\n",
    "        cap = cv.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(\"Error: Unable to open video file at path:\", video_path)\n",
    "            return None\n",
    "        all_landmarks = []\n",
    "\n",
    "        # Get video properties for output video\n",
    "        frame_width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = cap.get(cv.CAP_PROP_FPS)\n",
    "\n",
    "        # Define the codec and create VideoWriter object\n",
    "        out = None\n",
    "        if make_video:\n",
    "            output_video_name = output_video_name + '.mp4'\n",
    "            output_video_path = os.path.join(output_video_path, output_video_name)\n",
    "            out = cv.VideoWriter(output_video_path, cv.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "        while cap.isOpened():\n",
    "            success, image = cap.read()\n",
    "            if not success:\n",
    "                break\n",
    "\n",
    "            image_rgb = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "            results = pose.process(image_rgb)\n",
    "\n",
    "            if results.pose_landmarks:\n",
    "                # Draw the pose annotation on the image\n",
    "                mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "                # Extract and store landmarks\n",
    "                ref_x = (results.pose_landmarks.landmark[23].x + results.pose_landmarks.landmark[24].x) / 2\n",
    "                ref_y = (results.pose_landmarks.landmark[23].y + results.pose_landmarks.landmark[24].y) / 2\n",
    "                ref_z = (results.pose_landmarks.landmark[23].z + results.pose_landmarks.landmark[24].z) / 2\n",
    "\n",
    "                landmarks = []\n",
    "                for lm in results.pose_landmarks.landmark:\n",
    "                    rel_x, rel_y, rel_z = lm.x - ref_x, lm.y - ref_y, lm.z - ref_z\n",
    "                    landmarks.extend([rel_x, rel_y, rel_z, lm.visibility])\n",
    "                all_landmarks.append(landmarks)\n",
    "\n",
    "                if make_video:\n",
    "                    out.write(image)  # Write the frame with pose to the output video\n",
    "\n",
    "        cap.release()\n",
    "        if make_video:\n",
    "            out.release()\n",
    "\n",
    "        df = pd.DataFrame(all_landmarks)\n",
    "\n",
    "        #return df\n",
    "        skeletons_tensor = torch.tensor(all_landmarks, dtype=torch.float32)\n",
    "        print(skeletons_tensor.shape)\n",
    "        return skeletons_tensor\n",
    "    except:\n",
    "        return torch.Tensor([1])  # 오류 발생 시 기본 텐서 반환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skeleton_csv90frames(video_path='', output_video_path='', make_video=True, output_video_name='skeleton_video'):\n",
    "\n",
    "    \n",
    "    # try:\n",
    "    #     ffmpeg.probe(video_path)\n",
    "    # except:\n",
    "    #     print(f\"FFmpeg error for file {video_path}: {e.stderr}\")\n",
    "    #     return torch.Tensor([1])  # Return a default tensor if FFmpeg cannot read the file\n",
    "\n",
    "\n",
    "    # Initialize MediaPipe Pose\n",
    "    mp_pose = mp.solutions.pose\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, enable_segmentation=False, min_detection_confidence=0.5)\n",
    "\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Unable to open video file at path:\", video_path)\n",
    "        return torch.Tensor([1]) # prevent from None going in to dataset (if not, learning stops)\n",
    "    all_landmarks = []\n",
    "\n",
    "    # Get video properties for output video\n",
    "    frame_width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv.CAP_PROP_FPS)\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    out = None\n",
    "    if make_video: \n",
    "        output_video_name = output_video_name + '.mp4'\n",
    "        output_video_path = os.path.join(output_video_path, output_video_name)\n",
    "        out = cv.VideoWriter(output_video_path, cv.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "    frame_count = 0\n",
    "    max_frames = 30  # Process only first 90 frames\n",
    "\n",
    "    while cap.isOpened() and frame_count < max_frames:\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        image_rgb = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "        results = pose.process(image_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            # Draw the pose annotation on the image\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "\n",
    "            # Extract and store landmarks\n",
    "            ref_x = (results.pose_landmarks.landmark[23].x + results.pose_landmarks.landmark[24].x) / 2\n",
    "            ref_y = (results.pose_landmarks.landmark[23].y + results.pose_landmarks.landmark[24].y) / 2\n",
    "            ref_z = (results.pose_landmarks.landmark[23].z + results.pose_landmarks.landmark[24].z) / 2\n",
    "\n",
    "            landmarks = []\n",
    "            for lm in results.pose_landmarks.landmark:\n",
    "                rel_x, rel_y, rel_z = lm.x - ref_x, lm.y - ref_y, lm.z - ref_z\n",
    "                landmarks.extend([rel_x, rel_y, rel_z, lm.visibility])\n",
    "            all_landmarks.append(landmarks)\n",
    "\n",
    "            if make_video:\n",
    "                out.write(image)  # Write the frame with pose to the output video\n",
    "\n",
    "        frame_count += 1  # Increment frame count\n",
    "\n",
    "    cap.release()\n",
    "    if make_video:\n",
    "        out.release()\n",
    "\n",
    "\n",
    "    skeletons_tensor = torch.tensor(all_landmarks, dtype=torch.float32)\n",
    "    if skeletons_tensor is None:\n",
    "        skeletons_tensor = torch.Tensor([1]) # Prevent from None type return\n",
    "    return skeletons_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dataset - video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCF4Dataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        ds_store_path = os.path.join(directory, '.DS_Store')\n",
    "        if os.path.exists(ds_store_path):\n",
    "            os.remove(ds_store_path)\n",
    "        self.directory = directory\n",
    "        self.classes = os.listdir(directory)\n",
    "        self.data = []\n",
    "\n",
    "        # ignore ._ files  (Mac os system folder)\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(directory, class_name)\n",
    "            for video in os.listdir(class_path):\n",
    "                if not video.startswith('.'):\n",
    "                    self.data.append((os.path.join(class_path, video), class_name))\n",
    "        print(self.classes)\n",
    "        print(len(self.classes))\n",
    "        # MediaPipe 초기화\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.pose = self.mp_pose.Pose(static_image_mode=False, model_complexity=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path, class_name = self.data[idx]\n",
    "        skeletons = skeleton_csv(video_path=video_path,make_video=False)\n",
    "\n",
    "        label = self.classes.index(class_name)\n",
    "        return skeletons, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function simplifyer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_skeleton(video_path, pose):\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"비디오 파일을 열 수 없습니다:\", video_path)\n",
    "        return None\n",
    "\n",
    "    all_landmarks = []\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        image_rgb = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "        results = pose.process(image_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = []\n",
    "            for lm in results.pose_landmarks.landmark:\n",
    "                landmarks.extend([lm.x, lm.y, lm.z, lm.visibility])\n",
    "            all_landmarks.append(landmarks)\n",
    "\n",
    "    cap.release()\n",
    "    return all_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_relative_position(all_landmarks):\n",
    "    for landmarks in all_landmarks:\n",
    "        # 여기서는 예를 들어 힙 중심을 기준으로 상대적 위치를 계산합니다.\n",
    "        ref_x = (landmarks[23*4] + landmarks[24*4]) / 2\n",
    "        ref_y = (landmarks[23*4+1] + landmarks[24*4+1]) / 2\n",
    "        ref_z = (landmarks[23*4+2] + landmarks[24*4+2]) / 2\n",
    "\n",
    "        for i in range(0, len(landmarks), 4):\n",
    "            landmarks[i] -= ref_x\n",
    "            landmarks[i+1] -= ref_y\n",
    "            landmarks[i+2] -= ref_z\n",
    "\n",
    "    return all_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(all_landmarks, output_csv_path):\n",
    "    df = pd.DataFrame(all_landmarks)\n",
    "    df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_skeleton_video(video_path, output_video_path, pose):\n",
    "    # mp_drawing 모듈 로드\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "    cap = cv.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"비디오 파일을 열 수 없습니다:\", video_path)\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv.CAP_PROP_FPS)\n",
    "\n",
    "    out = cv.VideoWriter(output_video_path, cv.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        image_rgb = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
    "        results = pose.process(image_rgb)\n",
    "\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(image, results.pose_landmarks, mp.solutions.pose.POSE_CONNECTIONS)\n",
    "\n",
    "        out.write(image)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCF4Dataset_output_video(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        # define your self\n",
    "        #self.output_base_path = '..\\\\data\\\\UCF101\\\\UCF-101_outputvideo' #'..\\\\data\\\\UCF101\\\\UCF-101\n",
    "        self.output_base_path = '..\\\\data\\\\UCF101\\\\UCF-101 - skell' #'..\\\\data\\\\UCF101\\\\UCF-101\n",
    "\n",
    "        self.create_skeleton_video = 1\n",
    "        # MediaPipe 초기화\n",
    "        self.mp_pose = mp.solutions.pose\n",
    "        self.pose = self.mp_pose.Pose(static_image_mode=False, model_complexity=1)\n",
    "\n",
    "\n",
    "        # data cleaning\n",
    "        ds_store_path = os.path.join(directory, '.DS_Store')\n",
    "        if os.path.exists(ds_store_path):\n",
    "            os.remove(ds_store_path)\n",
    "        self.directory = directory\n",
    "        self.classes = os.listdir(directory)\n",
    "        self.data = []\n",
    "        \n",
    "\n",
    "        # ignore ._ files  (Mac os system folder)\n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(directory, class_name)\n",
    "            for video in os.listdir(class_path):\n",
    "                if not video.startswith('.'):\n",
    "                    self.data.append((os.path.join(class_path, video), class_name))\n",
    "        print(self.classes)\n",
    "        print(len(self.classes))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            video_path, class_name = self.data[idx]\n",
    "\n",
    "            # 원본 폴더 구조를 새로운 출력 폴더에 복제\n",
    "            relative_path = os.path.relpath(video_path, self.directory)\n",
    "            output_video_path = os.path.join(self.output_base_path, relative_path)\n",
    "            \n",
    "            # 출력 디렉토리가 존재하는지 확인, 없으면 생성\n",
    "            output_video_dir = os.path.dirname(output_video_path)\n",
    "            if not os.path.exists(output_video_dir):\n",
    "                os.makedirs(output_video_dir)\n",
    "\n",
    "            # 원본 비디오 경로를 기반으로 고유한 출력 비디오 이름 생성\n",
    "            base_name = os.path.basename(video_path)\n",
    "            name, ext = os.path.splitext(base_name)\n",
    "            ext = '.mp4'\n",
    "            output_video_name = f\"{name}_skeleton{ext}\"\n",
    "\n",
    "            # 출력 비디오 이름을 출력 경로에 추가\n",
    "            output_video_full_path = os.path.join(output_video_dir, output_video_name)\n",
    "\n",
    "            # 스켈레톤 데이터 추출\n",
    "            all_landmarks = extract_skeleton(video_path, self.pose)\n",
    "\n",
    "            # 상대 위치로 변환\n",
    "            all_landmarks = convert_to_relative_position(all_landmarks)\n",
    "\n",
    "            # 선택적으로 CSV 저장\n",
    "            # save_to_csv(all_landmarks, 'output.csv')\n",
    "\n",
    "            # 선택적으로 동영상 생성\n",
    "            if self.create_skeleton_video:\n",
    "                create_skeleton_video(video_path, output_video_full_path, self.pose)\n",
    "\n",
    "            # 텐서로 변환\n",
    "            skeletons_tensor = torch.tensor(all_landmarks, dtype=torch.float32)\n",
    "\n",
    "            label = self.classes.index(class_name)\n",
    "            return skeletons_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', 'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', 'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth', 'CleanAndJerk', 'CliffDiving', 'CricketBowling', 'CricketShot', 'CuttingInKitchen', 'Diving', 'Drumming', 'Fencing', 'FieldHockeyPenalty', 'FloorGymnastics', 'FrisbeeCatch', 'FrontCrawl', 'GolfSwing', 'Haircut', 'Hammering', 'HammerThrow', 'HandstandPushups', 'HandstandWalking', 'HeadMassage', 'HighJump', 'HorseRace', 'HorseRiding', 'HulaHoop', 'IceDancing', 'JavelinThrow', 'JugglingBalls', 'JumpingJack', 'JumpRope', 'Kayaking', 'Knitting', 'LongJump', 'Lunges', 'MilitaryParade', 'Mixing', 'MoppingFloor', 'Nunchucks', 'ParallelBars', 'PizzaTossing', 'PlayingCello', 'PlayingDaf', 'PlayingDhol', 'PlayingFlute', 'PlayingGuitar', 'PlayingPiano', 'PlayingSitar', 'PlayingTabla', 'PlayingViolin', 'PoleVault', 'PommelHorse', 'PullUps', 'Punch', 'PushUps', 'Rafting', 'RockClimbingIndoor', 'RopeClimbing', 'Rowing', 'SalsaSpin', 'ShavingBeard', 'Shotput', 'SkateBoarding', 'Skiing', 'Skijet', 'SkyDiving', 'SoccerJuggling', 'SoccerPenalty', 'StillRings', 'SumoWrestling', 'Surfing', 'Swing', 'TableTennisShot', 'TaiChi', 'TennisSwing', 'ThrowDiscus', 'TrampolineJumping', 'Typing', 'UnevenBars', 'VolleyballSpiking', 'WalkingWithDog', 'WallPushups', 'WritingOnBoard', 'YoYo']\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 및 데이터 로더 초기화\n",
    "#dataset_path = '..\\\\data\\\\UCF101\\\\UCF4 small'  # 소규모 추출된 데이터셋의 경로\n",
    "dataset_path = '..\\\\data\\\\UCF101\\\\UCF-101' #all dataset!\n",
    "\n",
    "\n",
    "#dataset = UCF4Dataset(dataset_path)\n",
    "dataset = UCF4Dataset_output_video(dataset_path) # Output the video too\n",
    "\n",
    "# 데이터셋 크기 정의\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(dataset_size * 0.7)  # 70%를 훈련 데이터로 사용\n",
    "val_size = int(dataset_size * 0.15)  # 15%를 검증 데이터로 사용\n",
    "test_size = dataset_size - train_size - val_size  # 나머지를 테스트 데이터로 사용\n",
    "\n",
    "# Split\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Create Dataset\n",
    "Batch_Size = 1\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=Batch_Size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=Batch_Size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=Batch_Size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9324, 1998, 1998)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader),len(val_loader),len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): LSTMModel(\n",
       "    (lstm): LSTM(132, 128, num_layers=5, batch_first=True)\n",
       "    (fc): Linear(in_features=128, out_features=101, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the LSTM model\n",
    "input_size = 132  # Adjust based on your skeleton data\n",
    "hidden_size = 128\n",
    "num_layers = 5\n",
    "num_classes = 4  # Adjust based on the number of classes in your dataset\n",
    "num_classes = 101 # Base on UCF101 origianl\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "if multi_gpu:\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=1, factor=0.5, min_lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables to track the best model\n",
    "best_val_loss = float('inf')\n",
    "best_model = None\n",
    "\n",
    "# Trackers for graph\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/9324 [03:02<32:32:01, 12.58s/it]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for skeletons, labels in tqdm(train_loader):\n",
    "        skeletons, labels = skeletons.to(device), labels.to(device)\n",
    "\n",
    "        try:\n",
    "            outputs = model(skeletons)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # 검증 루프\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for skeletons, labels in val_loader:\n",
    "            skeletons, labels = skeletons.to(device), labels.to(device)\n",
    "            try:\n",
    "                outputs = model(skeletons)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    # 최고 모델 저장\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        best_model = model.state_dict()\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss}, Val Loss: {avg_val_loss}')\n",
    "\n",
    "\n",
    "# Save the best model\n",
    "torch.save(best_model, '../OUTPUT/ucf101_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting train and validation losses\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loop for accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for skeletons, labels in test_loader:\n",
    "        try:\n",
    "            outputs = model(skeletons)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy on test set: {accuracy}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
